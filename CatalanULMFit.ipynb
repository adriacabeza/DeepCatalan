# -*- coding: utf-8 -*-
"""CatalanULMFit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1onmxyaGQSRz4bAmDBsLOZu9RGV-y4FAW

## Instal·lant dependències
"""

import os
from fastai import *
from fastai.text import *
from gensim.corpora import WikiCorpus

import numpy as np
import pandas as pd

from pathlib import Path
from functools import partial

!pip install sacremoses

!nvidia-smi

"""## Descarreguem el Wikipedia dump en català"""

!rm -rf data/wiki_extr
!rm -rf wikiextractor/ WikiExtractor.py

!sh get_wikimedia.sh

name = 'CatalanULMFit'
model_dir = 'models'
model_dir = Path(model_dir)
model_dir.mkdir(exist_ok=True)

"""Creem el corpus"""

!ls data/wiki_extr/ca

!python create_wikiText.py -i data/wiki_extr/ca -o data/wiki -l ca

!python postprocess_wikiText.py -i data/wiki -l ca

trn_path = Path('data/wiki/ca-2/ca.wiki.train.tokens')
val_path = Path('data/wiki/ca-2/ca.wiki.valid.tokens')

with open(trn_path, encoding='utf8') as f:
  text = f.readlines()

df_train = pd.DataFrame(
  {'content': np.array(text), 'is_valid': np.zeros(len(text))},
  columns=['content', 'is_valid'])

with open(val_path, encoding='utf8') as f:
      text2 = f.readlines()

df_valid = pd.DataFrame(
    {'content': np.array(text2), 'is_valid': np.ones(len(text2))},
    columns=['content', 'is_valid'])
  
df_regroup = pd.concat([df_train, df_valid])
df_regroup.to_csv('data/wiki/full_train.csv', header=None, index=None)

tokenizer = Tokenizer(lang='ca', n_cpus=6)
data_lm_full = (TextList.from_csv('data/wiki', csv_name='full_train.csv', cols=0, processor=[TokenizeProcessor(tokenizer=tokenizer), NumericalizeProcessor(max_vocab=60000)])
           #Inputs: all the text files in path
            .split_from_df(col=1)
           #We may have other temp folders that contain text files so we only keep what's in train and test
            .label_for_lm()           
           #We want to do a language model so we label accordingly
            .databunch(bs=32))

!du -H data/wiki/full_train.csv

itos = data_lm_full.train_ds.vocab.itos
print('Size of vocabulary:', len(itos))
print('First 10 words in vocab:', ', '.join([itos[i] for i in range(10)]))

torch.cuda.is_available()

"""# Entrenar el Language Model

Hem usat com a dataset tot el text que s'ha trobat en el repositori https://github.com/Softcatala/ca-text-corpus. També hem provat d'entrenar amb un dataset creat a partir del script **./get_wikimedia.sh** però ens tornava un dataset massa brut.
"""

learn = language_model_learner(data_lm_full,  arch=AWD_LSTM, drop_mult=0, callback_fns=ShowGraph)

# save vocabulary
print('Saving vocabulary...')
with open(os.path.join('data', 'itos.pkl'), 'wb') as f:
    pickle.dump(itos, f)

data_lm_full.show_batch()

learn.lr_find()

learn.recorder.plot(skip_start=0)

learn.fit_one_cycle(10, 1e-2, moms=(0.8, 0.7))

learn.fit_one_cycle(5, 5e-3, moms=(0.8, 0.7))

learn.fit_one_cycle(1, 5e-3, moms=(0.8, 0.7))

learn.save('model_60k-vocab')

np.exp(3.25)